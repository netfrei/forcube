
@article{buddenbohm,
  title = {{Erfolgskriterien f\"ur den Aufbau und nachhaltigen Betrieb Virtueller Forschungsumgebungen}},
  author = {Buddenbohm, Stefan and Enke, Harry and Hofmann, Matthias and Klar, Jochen and Neuroth, Heike and Schwiegelshohn, Uwe},
  pages = {37},
  file = {/Users/gerdgrasshoff/Zotero/storage/SWL5DSF5/Buddenbohm et al. - Erfolgskriterien für den Aufbau und nachhaltigen B.pdf},
  language = {de}
}

@article{burgelman2019a,
  title = {Open {{Science}}, {{Open Data}}, and {{Open Scholarship}}: {{European Policies}} to {{Make Science Fit}} for the {{Twenty}}-{{First Century}}},
  shorttitle = {Open {{Science}}, {{Open Data}}, and {{Open Scholarship}}},
  author = {Burgelman, Jean-Claude and Pascu, Corina and Szkuta, Katarzyna and Von Schomberg, Rene and Karalopoulos, Athanasios and Repanas, Konstantinos and Schouppe, Michel},
  year = {2019},
  volume = {2},
  publisher = {{Frontiers}},
  issn = {2624-909X},
  doi = {10.3389/fdata.2019.00043},
  abstract = {Open science will make science more efficient, reliable and responsive to societal challenges. The European Commission has sought to advance Open Science policy from its inception in a holistic and integrated way, covering all aspects of the research cycle from scientific discovery and review to sharing knowledge, publishing and outreach. We present the steps taken with a forward-looking perspective of the challenges laying ahead, in particular the necessary change of the rewards and incentives system for researchers (for which various actors are co-responsible and which goes beyond the mandate the European Commission). Finally, we discuss the promising role of Artificial Intelligence within an Open Science perspective.},
  file = {/Users/gerdgrasshoff/Zotero/storage/66SSGQQV/Burgelman et al. - 2019 - Open Science, Open Data, and Open Scholarship Eur.pdf},
  journal = {Frontiers in Big Data},
  language = {English}
}

@article{corti,
  title = {Managing {{anD Sharing reSearch Data}}},
  author = {Corti, Louise},
  pages = {8},
  file = {/Users/gerdgrasshoff/Zotero/storage/GHA6BYGP/Corti - Managing anD Sharing reSearch Data.pdf},
  language = {en}
}

@article{danielnust2018,
  title = {Opening {{Reproducible Research}}: A Research Project Website and Blog},
  shorttitle = {Opening {{Reproducible Research}}},
  author = {Daniel N{\"u}st and Marc Schutzeichel and Markus Konkol},
  year = {2018},
  month = nov,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.3908414},
  abstract = {All blog entries of the blog for the research project Opening Reproducible Research (o2r, see also Offene Reproduzierbare Forschung @ DFG GEPRIS), online at https://o2r.info. o2r was funded by the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) under project numbers PE 1632/10-1, KR 3930/3-1 and TR 864/6-1.},
  file = {/Users/gerdgrasshoff/Zotero/storage/IFS98B8R/Daniel Nüst et al. - 2018 - Opening Reproducible Research a research project .pdf},
  language = {eng}
}

@misc{florianthiery2018,
  title = {Taming {{Time}} \textendash{} {{Modelling}} Uncertainty as Reproducible {{Linked Open Data}}},
  author = {Florian Thiery and Allard Mees},
  year = {2018},
  month = sep,
  doi = {10.5281/zenodo.1402510},
  abstract = {It is a stroke of luck that the RGZM (R\"omisch-Germanisches Zentralmuseum Mainz) hosts already since the mid-1990s several online available databases containing millions of datasets, with content from many different archaeological disciplines. These databases where constructed in interdisciplinary transnational projects and include a lot of ``hidden archaeological assumptions'' in their relational data models. Especially short cutted relative chronological information and its dependencies are not modelled using transparent methods. The aim of our project is to make these hidden assumptions in archaeology visible and provide them as Linked Open Data to establish reproducible research as a fundament for Open Science. In particular the Samian Research database at the RGZM offers nearly 250'000 identified potter stamps, which are traditionally dated in a short cutted way. In Roman archaeology this is usually expressed by establishing ``absolute dates'' in well known ``from-to'' tables, whereas in reality, the situation is much more diffuse. In fact, Limes fortress dating is often done based on circular arguments. Samian stamps, for example, are being dated by sites, which on their own are again being dated by Samian. This paper focuses on modelling these circular dating arguments using a relative chronology based on Allen's interval algebra for temporal reasoning in the Academic Meta Tool (AMT) to create Linked Open Data for reproducible and transparent research. AMT allows us to create a fitting ontology and visualise the reasoning results in a web app for detecting errors and circular reasoning. As an example, we will take a deeper look into the relative chronological relationships of the Limes fortresses.},
  file = {/Users/gerdgrasshoff/Zotero/storage/84IZM6S2/Florian Thiery und Allard Mees - 2018 - Taming Time – Modelling uncertainty as reproducibl.pdf},
  language = {eng}
}

@misc{florianthiery2019,
  title = {{Dating Mechanism: Eine Linked Data Strategie zur interoperablen und nachvollziehbaren Modellierung relativer Chronologien am Beispiel s\"udgallischer Terra Sigillata in Limes-Abschnitten}},
  shorttitle = {{Dating Mechanism}},
  author = {Florian Thiery and Allard Mees},
  year = {2019},
  month = jan,
  doi = {10.5281/zenodo.2540374},
  abstract = {Seit Mitte der 1990er Jahre stellt das RGZM (R\"omisch-Germanisches Zentralmuseum in Mainz) webbasierte Datenbanken mit hunderttausenden von Datens\"atzen aus verschiedenen arch\"aologischen Disziplinen der Scientific Community zur Verf\"ugung. Diese historisch gewachsenen, zumeist relationalen Datenbanken wurden in interdisziplin\"aren, transnationalen Projekten erstellt und beinhalten oft ,,hidden assumptions'' hinsichtlich Datierungen. Das Ziel des ~Projekts ,,Dating Mechanism`` ist es, diese impliziten Informationen mittels semantischer Datenmodelle in Graphen sichtbar und verf\"ugbar zu machen und sie als Linked Open Data (LOD) zu teilen. Diese interoperablen Daten sollen im Sinne der ,,reproducible research`` als Grundlage f\"ur Open Science der Forschungscommunity zur Verf\"ugung stehen. Insbesondere die Samian Research Datenbank[1] des RGZM beinhaltet fast 250'000 identifizierte T\"opferstempel, die in der Arch\"aologie auf traditioneller Art datiert werden. Dies wird gew\"ohnlich dadurch realisiert, dass "absolute Daten" in "from-to tables " erstellt werden, wobei die Realit\"at viel komplexer ist. In der Tat basieren Datierungen von Terra Sigillata oft auf so genannten "dated sites" oder gar ganzen Limes-Abschnitten, die selbst von Terra Sigillata datiert werden. Man trifft also regelm\"a\ss ig auf Kreisargumentationen. Dazu werden als erster Schritt Zeitintervalle mit offenen Anfangs- oder fehlenden End-Datierungen in eine relative Chronologie transformiert. Dar\"uber hinaus k\"onnen Zusatzinformationen, wie der Anteil an T\"opfern, welche die Limes-Abschnitte gemeinsam haben, als Informationen f\"ur den Grad einer Verbindung (hier =Vagheit) zwischen zwei Limes-Abschnitten genutzt werden. In einem Reasoning-Prozess werden diese vagen Informationen ebenfalls mit ausgewertet um neue interferierte Information zu erzeugen, die bei der Beantwortung von Forschungsfragen behilflich sein k\"onnen. Zur Modellierung dieser Problematik, wird ein auf RDF (Resource Description Framework) basierendes semantisches Modell entwickelt, das zum Ziel hat, den Terra Sigillata -Datierprozess verifizierbar und transparent zu machen, um so neue Strategien f\"ur den maschinenlesbaren Zugriff \textendash{} hier als LOD - auf arch\"aologische Daten und Informationen in Form von Graphen zu erm\"oglichen. Dieser Vortrag zeigt die Modellierung von Datierungsargumenten in einer relativen Chronologie, basierend auf Allens Intervall Algebra [2], in Kombination mit absoluten Datierungen. Dazu werden zwei prototypische Tools benutzt. Zum einen, das vom RGZM erstellte Tool ,,Alligator`` [3] zur Transformation einer Korrespondenzanalyse in RDF-modellierte Allen -Intervalle und zum anderen das von mainzed in Kooperation mit dem RGZM und dem i3mainz entwickelte ,,Academic Meta Tool`` (AMT) [4]. AMT erm\"oglicht die Modellierung von Vagheit in RDF-Graphen und ein ,,temporal reasoning`` ~nach Allen, sowie eine Visualisierung des Graphen und Download der Daten zur Verwendung in anderen Tools wie Neo4J. Die Nutzung von Graphstrukturen erm\"oglicht es somit, Information und Interpretationen der hochgranulierten Daten zu modellieren, um so ,,hidden assumptions`` sichtbar zu machen und transparent bereitzustellen. Dar\"uber hinaus k\"onnen durch die Anwendung der Tools ,,Alligator`` und ,,AMT`` heterogene Daten und komplexe digitale Forschungsfragen im Umgang mit relativen Chronologien in den Datenrepositorien des RGZM standardisiert und interoperabel zur Verf\"ugung gestellt werden. Somit k\"onnen wir zeigen, dass Flexibilit\"at im Datierungsprozess und Interoperabilit\"at bei der Nachhaltigkeit und Wiederverwendung zwei Seiten eines Terra Sigillata\textendash Datensatzes sind. [1] http://rgzm.de/samian [2] https://doi.org/10.1145/182.358434 [3] https://rgzm.github.io/alligator/ [4] http://academic-meta-tool.xyz},
  file = {/Users/gerdgrasshoff/Zotero/storage/KGG6TUTL/Florian Thiery und Allard Mees - 2019 - Dating Mechanism Eine Linked Data Strategie zur i.pdf},
  language = {deu}
}

@misc{florianthiery2019a,
  title = {{Dating Mechanism: Eine Linked Data Strategie zur interoperablen und nachvollziehbaren Modellierung relativer Chronologien am Beispiel s\"udgallischer Terra Sigillata in Limes-Abschnitten}},
  shorttitle = {{Dating Mechanism}},
  author = {Florian Thiery and Allard Mees},
  year = {2019},
  month = jan,
  doi = {10.5281/zenodo.2540374},
  abstract = {Seit Mitte der 1990er Jahre stellt das RGZM (R\"omisch-Germanisches Zentralmuseum in Mainz) webbasierte Datenbanken mit hunderttausenden von Datens\"atzen aus verschiedenen arch\"aologischen Disziplinen der Scientific Community zur Verf\"ugung. Diese historisch gewachsenen, zumeist relationalen Datenbanken wurden in interdisziplin\"aren, transnationalen Projekten erstellt und beinhalten oft ,,hidden assumptions'' hinsichtlich Datierungen. Das Ziel des ~Projekts ,,Dating Mechanism`` ist es, diese impliziten Informationen mittels semantischer Datenmodelle in Graphen sichtbar und verf\"ugbar zu machen und sie als Linked Open Data (LOD) zu teilen. Diese interoperablen Daten sollen im Sinne der ,,reproducible research`` als Grundlage f\"ur Open Science der Forschungscommunity zur Verf\"ugung stehen. Insbesondere die Samian Research Datenbank[1] des RGZM beinhaltet fast 250'000 identifizierte T\"opferstempel, die in der Arch\"aologie auf traditioneller Art datiert werden. Dies wird gew\"ohnlich dadurch realisiert, dass "absolute Daten" in "from-to tables " erstellt werden, wobei die Realit\"at viel komplexer ist. In der Tat basieren Datierungen von Terra Sigillata oft auf so genannten "dated sites" oder gar ganzen Limes-Abschnitten, die selbst von Terra Sigillata datiert werden. Man trifft also regelm\"a\ss ig auf Kreisargumentationen. Dazu werden als erster Schritt Zeitintervalle mit offenen Anfangs- oder fehlenden End-Datierungen in eine relative Chronologie transformiert. Dar\"uber hinaus k\"onnen Zusatzinformationen, wie der Anteil an T\"opfern, welche die Limes-Abschnitte gemeinsam haben, als Informationen f\"ur den Grad einer Verbindung (hier =Vagheit) zwischen zwei Limes-Abschnitten genutzt werden. In einem Reasoning-Prozess werden diese vagen Informationen ebenfalls mit ausgewertet um neue interferierte Information zu erzeugen, die bei der Beantwortung von Forschungsfragen behilflich sein k\"onnen. Zur Modellierung dieser Problematik, wird ein auf RDF (Resource Description Framework) basierendes semantisches Modell entwickelt, das zum Ziel hat, den Terra Sigillata -Datierprozess verifizierbar und transparent zu machen, um so neue Strategien f\"ur den maschinenlesbaren Zugriff \textendash{} hier als LOD - auf arch\"aologische Daten und Informationen in Form von Graphen zu erm\"oglichen. Dieser Vortrag zeigt die Modellierung von Datierungsargumenten in einer relativen Chronologie, basierend auf Allens Intervall Algebra [2], in Kombination mit absoluten Datierungen. Dazu werden zwei prototypische Tools benutzt. Zum einen, das vom RGZM erstellte Tool ,,Alligator`` [3] zur Transformation einer Korrespondenzanalyse in RDF-modellierte Allen -Intervalle und zum anderen das von mainzed in Kooperation mit dem RGZM und dem i3mainz entwickelte ,,Academic Meta Tool`` (AMT) [4]. AMT erm\"oglicht die Modellierung von Vagheit in RDF-Graphen und ein ,,temporal reasoning`` ~nach Allen, sowie eine Visualisierung des Graphen und Download der Daten zur Verwendung in anderen Tools wie Neo4J. Die Nutzung von Graphstrukturen erm\"oglicht es somit, Information und Interpretationen der hochgranulierten Daten zu modellieren, um so ,,hidden assumptions`` sichtbar zu machen und transparent bereitzustellen. Dar\"uber hinaus k\"onnen durch die Anwendung der Tools ,,Alligator`` und ,,AMT`` heterogene Daten und komplexe digitale Forschungsfragen im Umgang mit relativen Chronologien in den Datenrepositorien des RGZM standardisiert und interoperabel zur Verf\"ugung gestellt werden. Somit k\"onnen wir zeigen, dass Flexibilit\"at im Datierungsprozess und Interoperabilit\"at bei der Nachhaltigkeit und Wiederverwendung zwei Seiten eines Terra Sigillata\textendash Datensatzes sind. [1] http://rgzm.de/samian [2] https://doi.org/10.1145/182.358434 [3] https://rgzm.github.io/alligator/ [4] http://academic-meta-tool.xyz},
  file = {/Users/gerdgrasshoff/Zotero/storage/9P56AGVF/Florian Thiery und Allard Mees - 2019 - Dating Mechanism Eine Linked Data Strategie zur i.pdf},
  language = {deu}
}

@book{genette1997,
  title = {Paratexts: {{Thresholds}} of {{Interpretation}}},
  shorttitle = {Paratexts},
  author = {Genette, Gerard},
  year = {1997},
  month = mar,
  publisher = {{Cambridge University Press}},
  abstract = {Paratexts are those liminal devices and conventions, both within and outside the book, that form part of the complex mediation between book, author, publisher and reader: titles, forewords, epigraphs and publishers' jacket copy are part of a book's private and public history. In this first English translation of Paratexts, G\'erard Genette shows how the special pragmatic status of paratextual declaration requires a carefully calibrated analysis of their illocutionary force. With clarity, precision and an extraordinary range of reference, Paratexts constitutes an encyclopedic survey of the customs and institutions as revealed in the borderlands of the text. Genette presents a global view of these liminal mediations and the logic of their relation to the reading public by studying each element as a literary function. Richard Macksey's foreword describes how the poetics of paratexts interact with more general questions of literature as a cultural institution, and situates Gennet's work in contemporary literary theory.},
  googlebooks = {AmWhQzemk2EC},
  isbn = {978-0-521-42406-6},
  language = {en}
}

@article{gray,
  title = {Data {{Repositories}} and {{Data Catalogues}}},
  author = {Gray, Stephen},
  pages = {4},
  file = {/Users/gerdgrasshoff/Zotero/storage/ZVAWFDTV/Gray - Data Repositories and Data Catalogues.pdf},
  language = {en}
}

@misc{ioannidis2020,
  title = {Fantastic {{PIDs}} and {{Where NOT}} to {{Find Them}}},
  author = {Ioannidis, Alex},
  year = {2020},
  month = jan,
  address = {{Lisbon}},
  doi = {10.5281/zenodo.3630591},
  abstract = {One would have thought we're probably not that far away from completing the Catalogue of Life, this vast encyclopedia of all animal and plant species, right? Not even close! Even though bio-taxonomists have tried for centuries to systematically document life, they couldn't even imagine the complications these dark ages of digital information brought to their noble cause. Apparently, chameleons, spiders, and big cats don't only hide in nature, but also behind pay-walled publications and badly scanned 18th-century manuscripts. Millions of figures, taxonomic descriptions, and collected materials end up lost, destroyed, and undiscoverable to the research community. This is a story of how Plazi, a non-profit association for bio-taxonomic literature, took up the task to liberate life from these PDF cages, give it a FAIR chance, and with the help of Zenodo put a PID on it and provide a safe and sustainable home. It is also a story on how, similar to wildlife, even data can disguise itself inside papers but eventually using the power of PIDs join the tree of life and make it bloom. We'll explore what these fantastic PIDs look like, and how we make it possible for anybody to find them even in the darkest corners of tropical rainforests and harsh deserts.},
  language = {eng}
}

@article{knuth1984a,
  title = {Literate {{Programming}}},
  author = {Knuth, D. E.},
  year = {1984},
  month = jan,
  volume = {27},
  pages = {97--111},
  publisher = {{Oxford Academic}},
  issn = {0010-4620},
  doi = {10.1093/comjnl/27.2.97},
  abstract = {Abstract.  The author and his associates have been experimenting for the past several years with a programming language and documentation system called WEB. Thi},
  file = {/Users/gerdgrasshoff/Zotero/storage/R8SQCCSY/Knuth - 1984 - Literate Programming.pdf;/Users/gerdgrasshoff/Zotero/storage/HDFQ3RQ8/343244.html},
  journal = {The Computer Journal},
  language = {en},
  number = {2}
}

@article{lerera,
  title = {{{PyTorch}}-{{BigGraph}}: {{A Large}}-Scale {{Graph Embedding System}}},
  author = {Lerer, Adam and Wu, Ledell and Shen, Jiajun and Lacroix, Timothee and Wehrstedt, Luca and Bose, Abhijit and Peysakhovich, Alex},
  pages = {12},
  abstract = {Graph embedding methods produce unsupervised node features from graphs that can then be used for a variety of machine learning tasks. Modern graphs, particularly in industrial applications, contain billions of nodes and trillions of edges, which exceeds the capability of existing embedding systems. We present PyTorch-BigGraph (PBG), an embedding system that incorporates several modifications to traditional multi-relation embedding systems that allow it to scale to graphs with billions of nodes and trillions of edges. PBG uses graph partitioning to train arbitrarily large embeddings on either a single machine or in a distributed environment. We demonstrate comparable performance with existing embedding systems on common benchmarks, while allowing for scaling to arbitrarily large graphs and parallelization on multiple machines. We train and evaluate embeddings on several large social network graphs as well as the full Freebase dataset, which contains over 100 million nodes and 2 billion edges.},
  file = {/Users/gerdgrasshoff/Zotero/storage/G8T5HF3L/Lerer et al. - PyTorch-BigGraph A Large-scale Graph Embedding Sy.pdf},
  language = {en}
}

@misc{nielsen2018,
  title = {Asclepias: {{Enabling Software Citation}} and {{Discovery}}},
  shorttitle = {Asclepias},
  author = {Nielsen, Lars Holm and Muench, August and Accomazzi, Alberto and Nowak, Krzysztof and Ioannidis, Alexandros and {Blanco-Cuaresma}, Sergi and Henneken, Edwin A. and Gonzalez Lopez, Jose Benito and Steffen, Julie},
  year = {2018},
  month = jun,
  doi = {10.5281/zenodo.1283381},
  abstract = {Our goal is to promote scientific software into an identifiable, citable, and preservable object. We are focusing on the needs of two of the most important roles researchers play in the scholarly ecosystem: authors of scholarly manuscripts and developers of scientific software. We are building a technical framework and promoting a set of social practices that will help to manage some of the problems associated with software citations, which include software versions, release specific authorship, synonymous object identifiers, and best practices for journal references markup.},
  file = {/Users/gerdgrasshoff/Zotero/storage/SSR635YM/Nielsen et al. - 2018 - Asclepias Enabling Software Citation and Discover.pdf},
  language = {eng}
}

@inproceedings{piotrowski2014,
  title = {The {{Labeling System}}: {{A New Approach}} to {{Overcome}} the {{Vocabulary Bottleneck}}},
  shorttitle = {The {{Labeling System}}},
  booktitle = {{{DH}}-{{CASE II}}: {{Collaborative Annotations}} on {{Shared Environments}}: Metadata, Tools and Techniques in the {{Digital Humanities}}},
  author = {Piotrowski, Michael and Colavizza, Giovanni and Thiery, Florian and Bruhn, Kai-Christian},
  year = {2014},
  month = sep,
  pages = {1--6},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2657480.2657482},
  abstract = {Shared controlled vocabularies are a prerequisite for collaborative annotation and semantic interchange. The creation and maintenance of such vocabularies is, however, time-consuming and expensive. The diversity of research questions in the humanities makes it virtually impossible to create shared controlled vocabularies that cover a wide range of potential applications and satisfy the needs of diverse stakeholders. In this paper we present a novel conceptual approach for mitigating these problems. We propose that projects define their own vocabularies as needed and link the vocabulary terms to one or more concepts in a reference thesaurus, so that the project-specific term effectively serves as a "label" for a set of shared concepts. We also describe the implementation of this approach in the Labeling System. The Labeling System is a Web application that allows users to easily import concepts or create SKOS vocabularies and link the vocabulary terms to concepts from one or more reference thesauri.},
  isbn = {978-1-4503-2981-1},
  series = {{{DH}}-{{CASE}} '14}
}

@article{stall2020,
  title = {Generalist {{Repository Comparison Chart}}},
  author = {Stall, Shelley and Martone, Maryann E. and Chandramouliswaran, Ishwar and Crosas, Merc{\`e} and Federer, Lisa and Gautier, Julian and Hahnel, Mark and Larkin, Jennie and Lowenberg, Daniella and Pfeiffer, Nicole and Sim, Ida and Smith, Tim and Van Gulick, Ana E. and Walker, Erin and Wood, Julie and Zaringhalam, Maryam and Zigoni, Alberto},
  year = {2020},
  month = jul,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.3946720},
  abstract = {The General Repository Comparison Chart and FAIRsharing Collection (https://fairsharing.org/collection/GeneralRepositoryComparison)~is an outcome of the NIH Workshop on the Role of Generalist Repositories to Enhance Data Discoverability and Reuse held 11-12 February 2020 (workshop summary).~ Following the workshop, representatives of the participating generalist repositories collaborated to develop a tool researchers could use to make decisions about selecting a general repository. We intend for the content to be dynamically updated through our partnership with FAIRsharing.~ As we work towards that goal, we currently have a static version of the comparison.~~ It is important to state that researchers should first determine if an appropriate domain repository exists for their research data.~ Tools such as FAIRsharing.org and re3data.org can help with this determination. If using a domain repository is not possible, then a researcher should review both the general repository chart and consider their own institutional repository as a possible location to store their data.~ Researchers need to comply with the requirements of their community, funder, country, publisher, and possibly others to ensure the best repository is selected.~ For those interested in continuing the discussion on the role of generalist repositories to enhance data discoverability and reuse, please consider joining the Research Data Alliance (RDA)~and particularly the joint RDA/Force11 FAIRsharing WG (https://www.rd-alliance.org/group/fairsharing-registry-connecting-data-policies-standards-databases.html).~ There is now an RDA Community to support conversation and a mailing list you join (https://www.rd-alliance.org/groups/generalist-repository-comparison-chart-management-group).~ Watch for relevant sessions in the upcoming plenaries.},
  file = {/Users/gerdgrasshoff/Zotero/storage/ETZDXWZS/Stall et al. - 2020 - Generalist Repository Comparison Chart.pdf}
}

@article{stall2020a,
  title = {Generalist {{Repository Comparison Chart}}},
  author = {Stall, Shelley and Martone, Maryann E. and Chandramouliswaran, Ishwar and Crosas, Merc{\`e} and Federer, Lisa and Gautier, Julian and Hahnel, Mark and Larkin, Jennie and Lowenberg, Daniella and Pfeiffer, Nicole and Sim, Ida and Smith, Tim and Van Gulick, Ana E. and Walker, Erin and Wood, Julie and Zaringhalam, Maryam and Zigoni, Alberto},
  year = {2020},
  month = jul,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.3946720},
  abstract = {The General Repository Comparison Chart and FAIRsharing Collection (https://fairsharing.org/collection/GeneralRepositoryComparison)~is an outcome of the NIH Workshop on the Role of Generalist Repositories to Enhance Data Discoverability and Reuse held 11-12 February 2020 (workshop summary).~ Following the workshop, representatives of the participating generalist repositories collaborated to develop a tool researchers could use to make decisions about selecting a general repository. We intend for the content to be dynamically updated through our partnership with FAIRsharing.~ As we work towards that goal, we currently have a static version of the comparison.~~ It is important to state that researchers should first determine if an appropriate domain repository exists for their research data.~ Tools such as FAIRsharing.org and re3data.org can help with this determination. If using a domain repository is not possible, then a researcher should review both the general repository chart and consider their own institutional repository as a possible location to store their data.~ Researchers need to comply with the requirements of their community, funder, country, publisher, and possibly others to ensure the best repository is selected.~ For those interested in continuing the discussion on the role of generalist repositories to enhance data discoverability and reuse, please consider joining the Research Data Alliance (RDA)~and particularly the joint RDA/Force11 FAIRsharing WG (https://www.rd-alliance.org/group/fairsharing-registry-connecting-data-policies-standards-databases.html).~ There is now an RDA Community to support conversation and a mailing list you join (https://www.rd-alliance.org/groups/generalist-repository-comparison-chart-management-group).~ Watch for relevant sessions in the upcoming plenaries.},
  file = {/Users/gerdgrasshoff/Zotero/storage/ELWDARQL/Stall et al. - 2020 - Generalist Repository Comparison Chart.pdf}
}

@article{stallshelley2020,
  title = {Generalist {{Repository Comparison Chart}}},
  author = {Stall, Shelley and Martone, Maryann E. and Chandramouliswaran, Ishwar and Crosas, Merc{\`e} and Federer, Lisa and Gautier, Julian and Hahnel, Mark and Larkin, Jennie and Lowenberg, Daniella and Pfeiffer, Nicole and Sim, Ida and Smith, Tim and Van Gulick, Ana E. and Walker, Erin and Wood, Julie and Zaringhalam, Maryam and Zigoni, Alberto},
  year = {2020},
  month = jul,
  publisher = {{Zenodo}},
  doi = {10.5281/ZENODO.3946720},
  abstract = {The General Repository Comparison Chart and FAIRsharing Collection (https://fairsharing.org/collection/GeneralRepositoryComparison) is an outcome of the NIH Workshop on the Role of Generalist Repositories to Enhance Data Discoverability and Reuse held 11-12 February 2020 (workshop summary). Following the workshop, representatives of the participating generalist repositories collaborated to develop a tool researchers could use to make decisions about selecting a general repository. We intend for the content to be dynamically updated through our partnership with FAIRsharing. As we work towards that goal, we currently have a static version of the comparison. It is important to state that researchers should first determine if an appropriate domain repository exists for their research data. Tools such as FAIRsharing.org and re3data.org can help with this determination. If using a domain repository is not possible, then a researcher should review both the general repository chart and consider their own institutional repository as a possible location to store their data. Researchers need to comply with the requirements of their community, funder, country, publisher, and possibly others to ensure the best repository is selected. {$<$}br{$>$} For those interested in continuing the discussion on the role of generalist repositories to enhance data discoverability and reuse, please consider joining the Research Data Alliance (RDA) and particularly the joint RDA/Force11 FAIRsharing WG (https://www.rd-alliance.org/group/fairsharing-registry-connecting-data-policies-standards-databases.html). There is now an RDA Community to support conversation and a mailing list you join (https://www.rd-alliance.org/groups/generalist-repository-comparison-chart-management-group). Watch for relevant sessions in the upcoming plenaries.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access}
}

@incollection{unold2019,
  title = {{Academic Meta Tool \textendash{} Ein Web-Tool zur Modellierung von Vagheit}},
  booktitle = {{Die Modellierung des Zweifels \textendash{} Schl\"usselideen und -konzepte zur graphbasierten Modellierung von Unsicherheiten.}},
  author = {Unold, Martin and Thiery, Florian and Mees, Allard},
  editor = {Kuzera, Andreas and W{\"u}bbena, Thorsten},
  year = {2019},
  address = {{Wolfenb\"uttel}},
  copyright = {CC BY-SA 4.0},
  file = {/Users/gerdgrasshoff/Zotero/storage/BJ378U3B/sb004_004.html},
  language = {de},
  series = {{Zeitschrift f\"ur digitale Geisteswissenschaften}}
}

@article{wilkinson2017,
  title = {Interoperability and {{FAIRness}} through a Novel Combination of {{Web}} Technologies},
  author = {Wilkinson, Mark D. and Verborgh, Ruben and Santos, Luiz Olavo Bonino da Silva and Clark, Tim and Swertz, Morris A. and Kelpin, Fleur D. L. and Gray, Alasdair J. G. and Schultes, Erik A. and van Mulligen, Erik M. and Ciccarese, Paolo and Kuzniar, Arnold and Gavai, Anand and Thompson, Mark and Kaliyaperumal, Rajaram and Bolleman, Jerven T. and Dumontier, Michel},
  year = {2017},
  month = apr,
  volume = {3},
  pages = {e110},
  publisher = {{PeerJ Inc.}},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.110},
  abstract = {Data in the life sciences are extremely diverse and are stored in a broad spectrum of repositories ranging from those designed for particular data types (such as KEGG for pathway data or UniProt for protein data) to those that are general-purpose (such as FigShare, Zenodo, Dataverse or EUDAT). These data have widely different levels of sensitivity and security considerations. For example, clinical observations about genetic mutations in patients are highly sensitive, while observations of species diversity are generally not. The lack of uniformity in data models from one repository to another, and in the richness and availability of metadata descriptions, makes integration and analysis of these data a manual, time-consuming task with no scalability. Here we explore a set of resource-oriented Web design patterns for data discovery, accessibility, transformation, and integration that can be implemented by any general- or special-purpose repository as a means to assist users in finding and reusing their data holdings. We show that by using off-the-shelf technologies, interoperability can be achieved atthe level of an individual spreadsheet cell. We note that the behaviours of this architecture compare favourably to the desiderata defined by the FAIR Data Principles, and can therefore represent an exemplar implementation of those principles. The proposed interoperability design patterns may be used to improve discovery and integration of both new and legacy data, maximizing the utility of all scholarly outputs.},
  file = {/Users/gerdgrasshoff/Zotero/storage/XQCJ4H62/Wilkinson et al. - 2017 - Interoperability and FAIRness through a novel comb.pdf;/Users/gerdgrasshoff/Zotero/storage/34XDPIU7/cs-110.html},
  journal = {PeerJ Computer Science},
  language = {en}
}

@misc{zotero-40374,
  title = {Steps in the {{Data Life Cycle}} | {{University}} of {{Virginia Library Research Data Services}} + {{Sciences}}},
  file = {/Users/gerdgrasshoff/Zotero/storage/H9DFIQYS/lifecycle.html},
  howpublished = {https://data.library.virginia.edu/data-management/lifecycle/}
}

@misc{zotero-40376,
  title = {Federal {{Agency Funding Guidelines}} | {{University}} of {{Virginia Library Research Data Services}} + {{Sciences}}},
  file = {/Users/gerdgrasshoff/Zotero/storage/IYYMIZ42/funding.html},
  howpublished = {https://data.library.virginia.edu/data-management/funding/}
}

@misc{zotero-40389,
  title = {The {{Home}} of {{Location Technology Innovation}} and {{Collaboration}} | {{OGC}}},
  file = {/Users/gerdgrasshoff/Zotero/storage/57AAHAWM/www.ogc.org.html},
  howpublished = {https://www.ogc.org/}
}

@misc{zotero-40391,
  title = {Paratexts: {{Thresholds}} of {{Interpretation}} - {{Gerard Genette}} - {{Google Books}}},
  file = {/Users/gerdgrasshoff/Zotero/storage/CLRRN38E/books.html},
  howpublished = {https://books.google.de/books?hl=en\&lr=\&id=AmWhQzemk2EC\&oi=fnd\&pg=PR11\&dq=G\%C3\%A9rard+Genette\&ots=4bwjsBoKLR\&sig=5E9nwpSouLjozXBSrTYao3W36Tg\&redir\_esc=y}
}

@misc{zotero-40395,
  title = {Annika {{Rockenberger}}, {{PhiN}} 76/2016: 20\textendash 60.},
  file = {/Users/gerdgrasshoff/Zotero/storage/FIGTYEX3/p76t2.html},
  howpublished = {http://web.fu-berlin.de/phin/phin76/p76t2.htm\#fz16}
}

@article{zotero-40434,
  title = {Effect of {{Remdesivir}} on {{Patients}} with {{COVID}}-19: {{A Network Meta}}-{{Analysis}} of {{Randomized Control Trials}}},
  shorttitle = {Effect of {{Remdesivir}} on {{Patients}} with {{COVID}}-19},
  doi = {10.1016/j.virusres.2020.198137},
  abstract = {Several randomized controlled trials (RCTs) were conducted to investigate the effect of remdesivir for patients with COVID-19, but their results were conflicting. Thus, we conducted a network meta-analysis comparing the rate of clinical improvement among patients with COVID-19 who received 5-day course of remdesivir versus 10-day course of remdesivir versus standard care. Our network meta-analysis of 4 randomized controlled trials demonstrated that the rate of clinical improvement was significantly higher in the 5-day remdesivir group and 10-day remdesivir group compared to standard care group (OR [95\% confidence interval [CI]] =1.89 [1.40-2.56], P {$<$}0.001, OR [95\% CI] =1.38 [1.15-1.66], P {$<$}0.001, respectively). In addition, the rate of clinical improvement was significantly higher in the 5-day remdesivir group compared to the 10-day remdesivir group (OR [95\% confidence interval [CI]] =1.37 [1.01-1.85], P =0.041). Our analysis demonstrated that the use of remdesivir for patients with COVID-19 was associated with the significantly higher clinical improvement rate compared with standard care alone.},
  file = {/Users/gerdgrasshoff/Zotero/storage/974MGI48/pub.html},
  language = {en},
  pmid = {32827627}
}


